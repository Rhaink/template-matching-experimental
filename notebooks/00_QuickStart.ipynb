{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide - Experimental Template Matching\n",
    "\n",
    "This notebook provides a quick introduction to the experimental template matching system.\n",
    "You'll learn how to:\n",
    "\n",
    "1. Load a pre-trained model\n",
    "2. Make predictions on new images\n",
    "3. Visualize landmarks and confidence scores\n",
    "4. Compare with ground truth\n",
    "5. Analyze prediction errors\n",
    "\n",
    "**Expected runtime: ~2 minutes**\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A trained model (use `train_experimental.py` to create one)\n",
    "- Test images and ground truth landmarks\n",
    "- Basic familiarity with numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Add project paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from core.experimental_predictor import ExperimentalLandmarkPredictor\n",
    "from tests.fixtures import create_synthetic_images, create_synthetic_landmarks, get_quick_test_data\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Create Test Data\n",
    "\n",
    "First, let's load the default configuration and create some synthetic test data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load default configuration\n",
    "config_path = PROJECT_ROOT / \"configs\" / \"default_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Patch size: {config['eigenpatches']['patch_size']}\")\n",
    "print(f\"  PCA components: {config['eigenpatches']['n_components']}\")\n",
    "print(f\"  Pyramid levels: {config['eigenpatches']['pyramid_levels']}\")\n",
    "print(f\"  Lambda shape: {config['landmark_predictor']['lambda_shape']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic test data for demonstration\n",
    "print(\"Creating synthetic test data...\")\n",
    "\n",
    "# Create training data\n",
    "train_images = create_synthetic_images(n_images=10, image_size=(128, 128), seed=42)\n",
    "train_landmarks = create_synthetic_landmarks(n_images=10, image_size=(128, 128), seed=42)\n",
    "\n",
    "# Create test data\n",
    "test_images = create_synthetic_images(n_images=3, image_size=(128, 128), seed=100)\n",
    "test_landmarks = create_synthetic_landmarks(n_images=3, image_size=(128, 128), seed=100)\n",
    "\n",
    "print(f\"Created {len(train_images)} training images and {len(test_images)} test images\")\n",
    "print(f\"Image size: {train_images[0].shape}\")\n",
    "print(f\"Landmarks per image: {len(train_landmarks[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Model (Quick Demo)\n",
    "\n",
    "For this demo, we'll quickly train a model on synthetic data. In practice, you would use `train_experimental.py` with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experimental predictor\n",
    "print(\"Initializing experimental landmark predictor...\")\n",
    "predictor = ExperimentalLandmarkPredictor(config=config)\n",
    "\n",
    "# Train the model (this will take ~30-60 seconds)\n",
    "print(\"Training model on synthetic data...\")\n",
    "predictor.train(train_images, train_landmarks)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Model statistics: {predictor.get_prediction_statistics()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make Predictions\n",
    "\n",
    "Now let's use the trained model to predict landmarks on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test images\n",
    "print(\"Making predictions on test images...\")\n",
    "\n",
    "results = []\n",
    "for i, test_image in enumerate(test_images):\n",
    "    print(f\"  Processing image {i+1}/{len(test_images)}...\")\n",
    "    \n",
    "    # Predict landmarks with detailed results\n",
    "    result = predictor.predict_landmarks(test_image, return_detailed=True)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"    Processing time: {result.processing_time:.3f} seconds\")\n",
    "    print(f\"    Iterations: {result.iterations}\")\n",
    "    print(f\"    Convergence error: {result.convergence_error:.3f}\")\n",
    "\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results\n",
    "\n",
    "Let's visualize the predictions compared to ground truth landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs ground truth\n",
    "fig, axes = plt.subplots(1, len(test_images), figsize=(15, 5))\n",
    "if len(test_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (image, result, gt_landmarks) in enumerate(zip(test_images, results, test_landmarks)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    \n",
    "    # Plot ground truth landmarks (green)\n",
    "    gt_x, gt_y = gt_landmarks[:, 0], gt_landmarks[:, 1]\n",
    "    ax.scatter(gt_x, gt_y, c='green', s=30, alpha=0.8, label='Ground Truth', marker='o')\n",
    "    \n",
    "    # Plot predicted landmarks (red)\n",
    "    pred_x, pred_y = result.landmarks[:, 0], result.landmarks[:, 1]\n",
    "    ax.scatter(pred_x, pred_y, c='red', s=30, alpha=0.8, label='Predicted', marker='x')\n",
    "    \n",
    "    # Draw lines connecting corresponding points\n",
    "    for j in range(len(gt_landmarks)):\n",
    "        ax.plot([gt_x[j], pred_x[j]], [gt_y[j], pred_y[j]], 'b-', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Compute and display error\n",
    "    errors = np.linalg.norm(result.landmarks - gt_landmarks, axis=1)\n",
    "    mean_error = np.mean(errors)\n",
    "    \n",
    "    ax.set_title(f'Test Image {i+1}\\nMean Error: {mean_error:.2f} pixels')\n",
    "    ax.set_xlabel('X coordinate')\n",
    "    ax.set_ylabel('Y coordinate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "all_errors = []\n",
    "for result, gt_landmarks in zip(results, test_landmarks):\n",
    "    errors = np.linalg.norm(result.landmarks - gt_landmarks, axis=1)\n",
    "    all_errors.extend(errors)\n",
    "\n",
    "all_errors = np.array(all_errors)\n",
    "print(f\"\\nPrediction Summary:\")\n",
    "print(f\"  Mean error: {np.mean(all_errors):.3f} Â± {np.std(all_errors):.3f} pixels\")\n",
    "print(f\"  Median error: {np.median(all_errors):.3f} pixels\")\n",
    "print(f\"  Min error: {np.min(all_errors):.3f} pixels\")\n",
    "print(f\"  Max error: {np.max(all_errors):.3f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Per-Landmark Performance\n",
    "\n",
    "Let's examine how well each individual landmark is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-landmark errors\n",
    "n_landmarks = len(test_landmarks[0])\n",
    "per_landmark_errors = [[] for _ in range(n_landmarks)]\n",
    "\n",
    "for result, gt_landmarks in zip(results, test_landmarks):\n",
    "    errors = np.linalg.norm(result.landmarks - gt_landmarks, axis=1)\n",
    "    for j, error in enumerate(errors):\n",
    "        per_landmark_errors[j].append(error)\n",
    "\n",
    "# Convert to numpy arrays and compute statistics\n",
    "landmark_stats = []\n",
    "for j in range(n_landmarks):\n",
    "    errors = np.array(per_landmark_errors[j])\n",
    "    stats = {\n",
    "        'landmark': j,\n",
    "        'mean_error': np.mean(errors),\n",
    "        'std_error': np.std(errors),\n",
    "        'min_error': np.min(errors),\n",
    "        'max_error': np.max(errors)\n",
    "    }\n",
    "    landmark_stats.append(stats)\n",
    "\n",
    "# Plot per-landmark performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot of mean errors\n",
    "landmark_indices = range(n_landmarks)\n",
    "mean_errors = [stats['mean_error'] for stats in landmark_stats]\n",
    "std_errors = [stats['std_error'] for stats in landmark_stats]\n",
    "\n",
    "bars = ax1.bar(landmark_indices, mean_errors, yerr=std_errors, capsize=5, alpha=0.7)\n",
    "ax1.set_xlabel('Landmark Index')\n",
    "ax1.set_ylabel('Mean Error (pixels)')\n",
    "ax1.set_title('Per-Landmark Mean Error')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight best and worst landmarks\n",
    "best_landmark = np.argmin(mean_errors)\n",
    "worst_landmark = np.argmax(mean_errors)\n",
    "bars[best_landmark].set_color('green')\n",
    "bars[worst_landmark].set_color('red')\n",
    "\n",
    "# Box plot of error distributions\n",
    "ax2.boxplot(per_landmark_errors, labels=landmark_indices)\n",
    "ax2.set_xlabel('Landmark Index')\n",
    "ax2.set_ylabel('Error (pixels)')\n",
    "ax2.set_title('Per-Landmark Error Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print landmark analysis\n",
    "print(f\"Per-Landmark Analysis:\")\n",
    "print(f\"  Best landmark: #{best_landmark} (mean error: {mean_errors[best_landmark]:.3f} pixels)\")\n",
    "print(f\"  Worst landmark: #{worst_landmark} (mean error: {mean_errors[worst_landmark]:.3f} pixels)\")\n",
    "print(f\"  Error range: {np.max(mean_errors) - np.min(mean_errors):.3f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Analysis\n",
    "\n",
    "If the model provides confidence scores, let's analyze their relationship with prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence scores (if available)\n",
    "try:\n",
    "    confidence_results = []\n",
    "    for test_image in test_images:\n",
    "        landmarks, confidence = predictor.predict_with_confidence(test_image)\n",
    "        confidence_results.append(confidence)\n",
    "    \n",
    "    # Plot confidence vs error relationship\n",
    "    all_confidences = []\n",
    "    all_errors = []\n",
    "    \n",
    "    for i, (result, gt_landmarks, confidence) in enumerate(zip(results, test_landmarks, confidence_results)):\n",
    "        errors = np.linalg.norm(result.landmarks - gt_landmarks, axis=1)\n",
    "        all_errors.extend(errors)\n",
    "        all_confidences.extend(confidence)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(all_confidences, all_errors, alpha=0.6)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Prediction Error (pixels)')\n",
    "    plt.title('Confidence vs Prediction Error')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(all_confidences, all_errors, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(all_confidences, p(all_confidences), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # Compute correlation\n",
    "    correlation = np.corrcoef(all_confidences, all_errors)[0, 1]\n",
    "    plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Confidence Analysis:\")\n",
    "    print(f\"  Mean confidence: {np.mean(all_confidences):.3f}\")\n",
    "    print(f\"  Confidence range: {np.min(all_confidences):.3f} - {np.max(all_confidences):.3f}\")\n",
    "    print(f\"  Confidence-Error correlation: {correlation:.3f}\")\n",
    "    \n",
    "    if correlation < -0.3:\n",
    "        print(\"  â Good: Higher confidence correlates with lower error\")\n",
    "    elif correlation > 0.3:\n",
    "        print(\"  â  Warning: Higher confidence correlates with higher error\")\n",
    "    else:\n",
    "        print(\"  â Neutral: Weak correlation between confidence and error\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Confidence analysis not available: {e}\")\n",
    "    print(\"This is normal for models without confidence prediction capability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Summary\n",
    "\n",
    "Let's summarize the model's performance and compare it with the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall performance metrics\n",
    "all_errors = []\n",
    "processing_times = []\n",
    "\n",
    "for result, gt_landmarks in zip(results, test_landmarks):\n",
    "    errors = np.linalg.norm(result.landmarks - gt_landmarks, axis=1)\n",
    "    mean_error = np.mean(errors)\n",
    "    all_errors.append(mean_error)\n",
    "    processing_times.append(result.processing_time)\n",
    "\n",
    "all_errors = np.array(all_errors)\n",
    "processing_times = np.array(processing_times)\n",
    "\n",
    "# Baseline comparison\n",
    "baseline_error = 5.63  # pixels (from original implementation)\n",
    "method_error = np.mean(all_errors)\n",
    "improvement = (baseline_error - method_error) / baseline_error * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENTAL TEMPLATE MATCHING - PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Training images: {len(train_images)}\")\n",
    "print(f\"  Test images: {len(test_images)}\")\n",
    "print(f\"  Image size: {test_images[0].shape}\")\n",
    "print(f\"  Landmarks per image: {len(test_landmarks[0])}\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Patch size: {config['eigenpatches']['patch_size']}\")\n",
    "print(f\"  PCA components: {config['eigenpatches']['n_components']}\")\n",
    "print(f\"  Pyramid levels: {config['eigenpatches']['pyramid_levels']}\")\n",
    "print(f\"  Lambda shape: {config['landmark_predictor']['lambda_shape']}\")\n",
    "print(f\"  Max iterations: {config['landmark_predictor']['max_iterations']}\")\n",
    "\n",
    "print(f\"\\nPerformance Results:\")\n",
    "print(f\"  Mean error: {method_error:.3f} Â± {np.std(all_errors):.3f} pixels\")\n",
    "print(f\"  Median error: {np.median(all_errors):.3f} pixels\")\n",
    "print(f\"  Error range: {np.min(all_errors):.3f} - {np.max(all_errors):.3f} pixels\")\n",
    "print(f\"  Mean processing time: {np.mean(processing_times):.3f} Â± {np.std(processing_times):.3f} seconds\")\n",
    "\n",
    "print(f\"\\nBaseline Comparison:\")\n",
    "print(f\"  Baseline error: {baseline_error:.2f} pixels\")\n",
    "print(f\"  Method error: {method_error:.3f} pixels\")\n",
    "print(f\"  Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "if improvement > 10:\n",
    "    performance_category = \"ð¢ Significantly Better\"\n",
    "elif improvement > 5:\n",
    "    performance_category = \"ð¢ Better\"\n",
    "elif improvement > -5:\n",
    "    performance_category = \"ð¡ Equivalent\"\n",
    "elif improvement > -10:\n",
    "    performance_category = \"ð  Worse\"\n",
    "else:\n",
    "    performance_category = \"ð´ Significantly Worse\"\n",
    "\n",
    "print(f\"  Category: {performance_category}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  ð Use evaluate_experimental.py for comprehensive analysis\")\n",
    "print(f\"  ð¬ Explore 01_Mathematical_Analysis.ipynb for deeper insights\")\n",
    "print(f\"  â¡ Try different configurations in experimental_configs.yaml\")\n",
    "print(f\"  ð§ª Run parameter sensitivity analysis with 04_Parameter_Sensitivity.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results (Optional)\n",
    "\n",
    "Optionally save the model and results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results (optional)\n",
    "save_results = input(\"Save model and results? (y/n): \").lower().startswith('y')\n",
    "\n",
    "if save_results:\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = PROJECT_ROOT / \"results\" / \"quickstart\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = output_dir / f\"quickstart_model_{timestamp}.pkl\"\n",
    "    predictor.save(str(model_path))\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_data = {\n",
    "        'config': config,\n",
    "        'test_results': results,\n",
    "        'test_landmarks': test_landmarks,\n",
    "        'performance_summary': {\n",
    "            'mean_error': float(method_error),\n",
    "            'std_error': float(np.std(all_errors)),\n",
    "            'baseline_comparison': float(improvement),\n",
    "            'performance_category': performance_category\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = output_dir / f\"quickstart_results_{timestamp}.pkl\"\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results_data, f)\n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Results not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "ð **Congratulations!** You've successfully:\n",
    "\n",
    "â Loaded and configured the experimental template matching system  \n",
    "â Trained a model on synthetic data  \n",
    "â Made predictions and visualized results  \n",
    "â Analyzed per-landmark performance  \n",
    "â Compared with baseline performance  \n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **ð Mathematical Analysis**: Explore `01_Mathematical_Analysis.ipynb` for deep mathematical insights\n",
    "- **ð Results Analysis**: Use `02_Results_Analysis.ipynb` to analyze real experimental results\n",
    "- **ð§ª Experimentation**: Try `03_Prompt_Driven_Experiments.ipynb` for AI-assisted research\n",
    "- **âï¸ Parameter Tuning**: Use `04_Parameter_Sensitivity.ipynb` to optimize performance\n",
    "\n",
    "### Production Usage\n",
    "\n",
    "For production use with real data:\n",
    "\n",
    "1. **Training**: Use `train_experimental.py` with your coordinate files\n",
    "2. **Processing**: Use `process_experimental.py` to process test images\n",
    "3. **Evaluation**: Use `evaluate_experimental.py` for comprehensive analysis\n",
    "4. **Pipeline**: Use `run_full_pipeline.py` for automated workflows\n",
    "\n",
    "**Happy experimenting! ð**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}